{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":13863472,"sourceType":"datasetVersion","datasetId":8832308}],"dockerImageVersionId":31193,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"\npip install gym-super-mario-bros\npip install tensordict\npip install torchrl","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import warnings\nwarnings.filterwarnings('ignore')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nfrom torch import nn\nfrom torchvision import transforms as T\nfrom PIL import Image\nimport numpy as np\nfrom pathlib import Path\nfrom collections import deque\nimport random, datetime, os\n\nimport gym\nfrom gym.spaces import Box\nfrom gym.wrappers import FrameStack\n\nfrom nes_py.wrappers import JoypadSpace\n\nimport gym_super_mario_bros\n\nfrom tensordict import TensorDict\nfrom torchrl.data import TensorDictReplayBuffer, LazyMemmapStorage\nimport numpy as np\nimport time,datetime\nimport matplotlib.pyplot as plt\nimport pandas as pd\nfrom pathlib import Path\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\nif gym.__version__<'0.26':\n    env=gym_super_mario_bros.make(\"SuperMarioBros-1-1-v0\",new_step_api=True)\nelse:\n    env=gym_super_mario_bros.make(\"SuperMarioBros-1-1-v0\", render_mode='rgb', apply_api_compatibility=True)\n\nenv=JoypadSpace(env, [[\"right\"], [\"right\", \"A\"]])\n\nenv.reset()\nnext_state, reward, done, trunc, info=env.step(action=0)\nprint(f\"{next_state.shape},\\n {reward},\\n {done},\\n {info}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class SkipFrame(gym.Wrapper):\n    def __init__(self,env,skip):\n        \"\"\"Return only every `skip`-th frame\"\"\"\n        super().__init__(env)\n        self._skip=skip\n\n    def step(self, action):\n   \n        total_reward=0.0\n        for i in range(self._skip):\n            # Accumulate reward and repeat the same action\n            obs, reward, done, trunk, info = self.env.step(action)\n            total_reward+=reward\n            if done:\n                break\n        return obs, total_reward, done, trunk, info","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class GrayScaleObservation(gym.ObservationWrapper):\n    def __init__(self,env):\n        super().__init__(env)\n        obs_shape=self.observation_space.shape[:2]\n        self.observation_space=Box(low=0,high=255,shape=obs_shape,dtype=np.uint8)\n\n    def permute_orientation(self, observation):\n        observation=np.transpose(observation, (2, 0, 1))\n        observation=torch.tensor(observation.copy(), dtype=torch.float)\n        return observation\n\n    def observation(self, observation):\n        observation=self.permute_orientation(observation)\n        transform=T.Grayscale()\n        observation=transform(observation)\n        return observation","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class ResizeObservation(gym.ObservationWrapper):\n    def __init__(self, env, shape):\n        super().__init__(env)\n        if isinstance(shape, int):\n            self.shape=(shape, shape)\n        else:\n            self.shape = tuple(shape)\n\n        obs_shape=self.shape+self.observation_space.shape[2:]\n        self.observation_space=Box(low=0, high=255, shape=obs_shape, dtype=np.uint8)\n\n    def observation(self, observation):\n        transforms=T.Compose(\n            [T.Resize(self.shape, antialias=True), T.Normalize(0,255)]\n        )\n        observation = transforms(observation).squeeze(0)\n        return observation","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class CustomReward(gym.Wrapper):\n    def __init__(self, env):\n        super().__init__(env)\n        self.max_x = 0\n        self.stuck_steps = 0\n\n    def reset(self, **kwargs):\n        self.max_x = 0\n        self.stuck_steps = 0\n        return self.env.reset(**kwargs)\n\n    def step(self, action):\n        state, reward, done, trunc, info = self.env.step(action)\n        \n        \n        x_pos = info.get(\"x_pos\", 0)\n\n        \n        if x_pos > self.max_x:\n            self.max_x = x_pos\n            self.stuck_steps = 0  \n        else:\n            self.stuck_steps += 1 \n\n        if self.stuck_steps > 50:\n            reward -= 1.0\n            \n       \n        if done and not info.get(\"flag_get\", False):\n            reward -= 50\n            \n        reward -= 0.1 \n\n        return state, reward, done, trunc, info","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"env=SkipFrame(env, skip=4)\nenv=GrayScaleObservation(env)\nenv = CustomReward(env)\nenv=ResizeObservation(env, shape=84)\nif gym.__version__ <'0.26':\n    env=FrameStack(env, num_stack=4, new_step_api=True)\nelse:\n    env=FrameStack(env, num_stack=4)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class Mario:\n    def __init__():\n        pass\n\n    def act(self,state):\n        pass\n\n    def cache(self,experience):\n        pass\n\n    def recall(self):\n        pass\n\n    def learn(self):\n        pass\n              ","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class Mario:\n    def __init__(self,state_dim,action_dim,save_dir):\n        self.state_dim=state_dim\n        self.action_dim=action_dim\n        self.save_dir=save_dir\n\n        self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n\n        self.net=MarioNet(self.state_dim, self.action_dim).float()\n        self.net=self.net.to(device=self.device)\n\n        self.exploration_rate=1\n        self.exploration_rate_decay=0.9999995 \n        self.exploration_rate_min=0.1\n        self.curr_step=0\n\n        self.save_every=5e5 \n\n    def act(self, state):\n        \"\"\"\n    Given a state, choose an epsilon-greedy action and update value of step.\n\n    Inputs:\n    state(``LazyFrame``): A single observation of the current state, dimension is (state_dim)\n    Outputs:\n    ``action_idx`` (``int``): An integer representing which action Mario will perform\n    \"\"\"\n\n        if np.random.rand()<self.exploration_rate:\n            action_idx=np.random.randint(self.action_dim)\n\n       \n        else:\n            state=state[0].__array__() if isinstance(state, tuple) else state.__array__()\n            state=torch.tensor(state, device=self.device).unsqueeze(0)\n            action_values=self.net(state, model=\"online\")\n            action_idx=torch.argmax(action_values, axis=1).item()\n\n       \n        self.exploration_rate *=self.exploration_rate_decay\n        self.exploration_rate=max(self.exploration_rate_min, self.exploration_rate)\n\n       \n        self.curr_step +=1\n        return action_idx","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class Mario(Mario):  \n    def __init__(self, state_dim, action_dim, save_dir):\n        super().__init__(state_dim, action_dim, save_dir)\n        self.memory=TensorDictReplayBuffer(storage=LazyMemmapStorage(60000, device=torch.device(\"cpu\")))\n        self.batch_size=32\n\n    def cache(self, state, next_state, action, reward, done):\n        \"\"\"\n        Store the experience to self.memory (replay buffer)\n\n        Inputs:\n        state (``LazyFrame``),\n        next_state (``LazyFrame``),\n        action (``int``),\n        reward (``float``),\n        done(``bool``))\n        \"\"\"\n        def first_if_tuple(x):\n            return x[0] if isinstance(x, tuple) else x\n        state=first_if_tuple(state).__array__()\n        next_state=first_if_tuple(next_state).__array__()\n\n        state=torch.tensor(state)\n        next_state=torch.tensor(next_state)\n        action=torch.tensor([action])\n        reward=torch.tensor([reward])\n        done=torch.tensor([done])\n\n       \n        self.memory.add(TensorDict({\"state\": state, \"next_state\": next_state, \"action\": action, \"reward\": reward, \"done\": done}, batch_size=[]))\n\n    def recall(self):\n        \"\"\"\n        Retrieve a batch of experiences from memory\n        \"\"\"\n        batch=self.memory.sample(self.batch_size).to(self.device)\n        state, next_state, action, reward, done = (batch.get(key) for key in (\"state\", \"next_state\", \"action\", \"reward\", \"done\"))\n        return state, next_state, action.squeeze(), reward.squeeze(), done.squeeze()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class MarioNet(nn.Module):\n    \"\"\"mini CNN structure\n  input -> (conv2d + relu) x 3 -> flatten -> (dense + relu) x 2 -> output\n  \"\"\"\n\n    def __init__(self, input_dim, output_dim):\n        super().__init__()\n        c, h, w=input_dim\n\n        if h!=84:\n            raise ValueError(f\"Expecting input height: 84, got: {h}\")\n        if w!=84:\n            raise ValueError(f\"Expecting input width: 84, got: {w}\")\n\n        self.online=self.__build_cnn(c, output_dim)\n\n        self.target=self.__build_cnn(c, output_dim)\n        self.target.load_state_dict(self.online.state_dict())\n\n        \n        for p in self.target.parameters():\n            p.requires_grad=False\n\n    def forward(self, input, model):\n        if model==\"online\":\n            return self.online(input)\n        elif model==\"target\":\n            return self.target(input)\n\n    def __build_cnn(self, c, output_dim):\n        return nn.Sequential(\n            nn.Conv2d(in_channels=c, out_channels=32, kernel_size=8, stride=4),\n            nn.ReLU(),\n            nn.Conv2d(in_channels=32, out_channels=64, kernel_size=4, stride=2),\n            nn.ReLU(),\n            nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, stride=1),\n            nn.ReLU(),\n            nn.Flatten(),\n            nn.Linear(3136, 512),\n            nn.ReLU(),\n            nn.Linear(512, output_dim),\n        )","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class Mario(Mario):\n    def __init__(self, state_dim, action_dim, save_dir):\n        super().__init__(state_dim, action_dim, save_dir)\n        self.gamma=0.9\n\n    def td_estimate(self, state, action):\n        current_Q=self.net(state, model=\"online\")[\n            np.arange(0, self.batch_size), action\n        ] \n        return current_Q\n\n    @torch.no_grad()\n    def td_target(self, reward, next_state, done):\n        next_state_Q=self.net(next_state, model=\"online\")\n        best_action=torch.argmax(next_state_Q, axis=1)\n        next_Q=self.net(next_state, model=\"target\")[\n            np.arange(0, self.batch_size), best_action\n        ]\n        return (reward +(1 - done.float()) * self.gamma * next_Q).float()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class Mario(Mario):\n    def __init__(self, state_dim, action_dim, save_dir):\n        super().__init__(state_dim, action_dim, save_dir)\n        self.optimizer=torch.optim.Adam(self.net.parameters(), lr=0.00025)\n        self.loss_fn=torch.nn.SmoothL1Loss()\n\n    def update_Q_online(self, td_estimate, td_target):\n        loss=self.loss_fn(td_estimate, td_target)\n        self.optimizer.zero_grad()\n        loss.backward()\n        self.optimizer.step()\n        return loss.item()\n\n    def sync_Q_target(self):\n        self.net.target.load_state_dict(self.net.online.state_dict())","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class Mario(Mario):\n    def save(self):\n        save_path=(self.save_dir / f\"mario_net_{int(self.curr_step // self.save_every)}.chkpt\")\n        torch.save(\n            dict(model=self.net.state_dict(), exploration_rate=self.exploration_rate),\n            save_path,\n        )\n        print(f\"MarioNet saved to {save_path} at step {self.curr_step}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class Mario(Mario):\n    def __init__(self, state_dim, action_dim, save_dir):\n        super().__init__(state_dim, action_dim, save_dir)\n        self.burnin=1e4  \n        self.learn_every=3 \n        self.sync_every=1e4 \n\n    def learn(self):\n        if self.curr_step % self.sync_every==0:\n            self.sync_Q_target()\n\n        if self.curr_step % self.save_every==0:\n            self.save()\n\n        if self.curr_step < self.burnin:\n            return None, None\n\n        if self.curr_step % self.learn_every!=0:\n            return None, None\n\n       \n        state, next_state, action, reward, done=self.recall()\n        \n        td_est=self.td_estimate(state, action)\n\n        td_tgt=self.td_target(reward, next_state, done)\n\n        loss=self.update_Q_online(td_est, td_tgt)\n\n        return (td_est.mean().item(), loss)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\n\nclass MetricLogger:\n    def __init__(self, save_dir: Path):\n        save_dir.mkdir(parents=True, exist_ok=True)\n\n        self.save_log = save_dir / \"log\"\n        \n        if not self.save_log.exists():\n            with open(self.save_log, \"w\") as f:\n                f.write(\n                    f\"{'Episode':>8}{'Step':>8}{'Epsilon':>10}{'MeanReward':>15}\"\n                    f\"{'MeanLength':>15}{'MeanLoss':>15}{'MeanQValue':>15}\"\n                    f\"{'TimeDelta':>15}{'Time':>20}\\n\"\n                )\n        \n     \n        self.ep_rewards = []\n        self.ep_lengths = []\n        self.ep_avg_losses = []\n        self.ep_avg_qs = []\n\n        \n        self.moving_avg_ep_rewards = []\n        self.moving_avg_ep_lengths = []\n        self.moving_avg_ep_avg_losses = []\n        self.moving_avg_ep_avg_qs = []\n\n        self.init_episode()\n        self.record_time = time.time()\n\n    def init_episode(self):\n        \"\"\"Reset current episode stats.\"\"\"\n        self.curr_ep_reward = 0.0\n        self.curr_ep_length = 0\n        self.curr_ep_loss = 0.0\n        self.curr_ep_q = 0.0\n        self.curr_ep_loss_length = 0\n\n    def log_step(self, reward, loss, q):\n        \"\"\"\n        Call this at every environment step.\n        - reward: scalar reward from env\n        - loss: training loss (or None if no update done on this step)\n        - q: mean Q-value for action (or None if not computed)\n        \"\"\"\n        self.curr_ep_reward += reward\n        self.curr_ep_length += 1\n\n        # Use \"is not None\" so we don't skip loss = 0.0\n        if loss is not None:\n            self.curr_ep_loss += float(loss)\n            if q is not None:\n                self.curr_ep_q += float(q)\n            self.curr_ep_loss_length += 1\n\n    def log_episode(self):\n        \"\"\"Call once at the end of an episode.\"\"\"\n        self.ep_rewards.append(self.curr_ep_reward)\n        self.ep_lengths.append(self.curr_ep_length)\n\n        if self.curr_ep_loss_length == 0:\n            ep_avg_loss = 0.0\n            ep_avg_q = 0.0\n        else:\n            ep_avg_loss = np.round(self.curr_ep_loss / self.curr_ep_loss_length, 5)\n            ep_avg_q = np.round(self.curr_ep_q / self.curr_ep_loss_length, 5)\n\n        self.ep_avg_losses.append(ep_avg_loss)\n        self.ep_avg_qs.append(ep_avg_q)\n\n        self.init_episode()\n\n    def record(self, episode, epsilon, step, log_to_console=False):\n        \"\"\"\n        Call once per episode AFTER log_episode().\n        - If log_to_console is True, prints summary.\n        - Always writes to file.\n        \"\"\"\n        mean_ep_reward = np.round(np.mean(self.ep_rewards[-100:]), 3)\n        mean_ep_length = np.round(np.mean(self.ep_lengths[-100:]), 3)\n        mean_ep_loss = np.round(np.mean(self.ep_avg_losses[-100:]), 3)\n        mean_ep_q = np.round(np.mean(self.ep_avg_qs[-100:]), 3)\n        \n        self.moving_avg_ep_rewards.append(mean_ep_reward)\n        self.moving_avg_ep_lengths.append(mean_ep_length)\n        self.moving_avg_ep_avg_losses.append(mean_ep_loss)\n        self.moving_avg_ep_avg_qs.append(mean_ep_q)\n\n        last_record_time = self.record_time\n        self.record_time = time.time()\n        time_since_last_record = np.round(self.record_time - last_record_time, 3)\n\n        if log_to_console:\n            print(\n                f\"Episode {episode} - \"\n                f\"Step {step} - \"\n                f\"Epsilon {epsilon:.4f} - \"\n                f\"Mean Reward {mean_ep_reward} - \"\n                f\"Mean Length {mean_ep_length} - \"\n                f\"Mean Loss {mean_ep_loss} - \"\n                f\"Mean Q Value {mean_ep_q} - \"\n                f\"Time Delta {time_since_last_record} - \"\n                f\"Time {datetime.datetime.now().strftime('%Y-%m-%dT%H:%M:%S')}\"\n            )\n\n        with open(self.save_log, \"a\") as f:\n            f.write(\n                f\"{episode:8d}{step:8d}{epsilon:10.3f}\"\n                f\"{mean_ep_reward:15.3f}{mean_ep_length:15.3f}{mean_ep_loss:15.3f}{mean_ep_q:15.3f}\"\n                f\"{time_since_last_record:15.3f}\"\n                f\"{datetime.datetime.now().strftime('%Y-%m-%dT%H:%M:%S'):>20}\\n\"\n            )\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"use_cuda = torch.cuda.is_available()\ndevice = torch.device(\"cuda\" if use_cuda else \"cpu\")\nprint(\"Using device:\", device)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# TRAINING","metadata":{}},{"cell_type":"code","source":"import torch\nfrom pathlib import Path\nimport gc\n\n\nuse_cuda = torch.cuda.is_available()\ndevice = torch.device(\"cuda\" if use_cuda else \"cpu\")\nprint(\"Using device:\", device)\n\nsave_dir = Path(\"/kaggle/working\")\n\nmetric_log_dir = save_dir / \"log\"\nmetric_logger = MetricLogger(metric_log_dir)\n\n\nmario = Mario(\n    state_dim=(4, 84, 84),\n    action_dim=env.action_space.n,\n    save_dir=save_dir\n)\n\nepisodes = 8000\n\nlog_file = save_dir / \"training_log.txt\"\n\nprint(f\"Starting Training for {episodes} episodes...\")\n\nfor e in range(episodes):\n    state = env.reset()\n    ep_reward = 0.0\n\n    metric_logger.init_episode()\n\n    done = False\n    while not done:\n        action = mario.act(state)\n\n        step_result = env.step(action)\n        if len(step_result) == 5:\n            next_state, reward, terminated, truncated, info = step_result\n            done = terminated or truncated\n        else:\n            next_state, reward, done, info = step_result\n\n        mario.cache(state, next_state, action, reward, done)\n\n        q, loss = mario.learn()\n\n        metric_logger.log_step(reward, loss, q)\n\n        ep_reward += reward\n        state = next_state\n\n        if info.get(\"flag_get\", False):\n            break\n\n    metric_logger.log_episode()\n    metric_logger.record(\n        episode=e,\n        epsilon=mario.exploration_rate,\n        step=mario.curr_step,\n        log_to_console=False     \n    )\n\n    if e % 20 == 0:\n        print(f\"Episode {e}/{episodes} | Step {mario.curr_step} | \"\n              f\"Reward {ep_reward:.1f} | Epsilon {mario.exploration_rate:.4f}\")\n        \n        with open(log_file, \"a\") as f:\n            f.write(f\"{e},{mario.curr_step},{ep_reward},{mario.exploration_rate}\\n\")\n\n        if e > 0 and e % 50 == 0:\n            save_path = save_dir / \"mario_net.pth\"\n            torch.save(mario.net.state_dict(), save_path)\n\n        if e > 0 and e % 500 == 0:\n            torch.save(mario.net.state_dict(), save_dir / f\"mario_ep_{e}.pth\")\n\n        if use_cuda:\n            torch.cuda.empty_cache()\n        gc.collect()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\ntorch.save(mario.net.state_dict(), \"mario_net_checkpoint.pth\")\nprint(\"Saved mario_net_checkpoint.pth successfully!\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## TESTING","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport numpy as np\nimport imageio\nimport warnings\nfrom torchvision import transforms as T\nfrom gym.spaces import Box\nfrom gym.wrappers import FrameStack\nimport gym_super_mario_bros\nfrom gym_super_mario_bros.actions import SIMPLE_MOVEMENT\nfrom nes_py.wrappers import JoypadSpace\nimport gym\n\nwarnings.filterwarnings('ignore')\n\n\nclass SkipFrame(gym.Wrapper):\n    def __init__(self, env, skip):\n        super().__init__(env)\n        self._skip = skip\n    def step(self, action):\n        total_reward = 0.0\n        for i in range(self._skip):\n            obs, reward, done, trunc, info = self.env.step(action)\n            total_reward += reward\n            if done: break\n        return obs, total_reward, done, trunc, info\n\nclass GrayScaleObservation(gym.ObservationWrapper):\n    def __init__(self, env):\n        super().__init__(env)\n        obs_shape = self.observation_space.shape[:2]\n        self.observation_space = Box(low=0, high=255, shape=obs_shape, dtype=np.uint8)\n    def permute_orientation(self, observation):\n        observation = np.transpose(observation, (2, 0, 1))\n        observation = torch.tensor(observation.copy(), dtype=torch.float)\n        return observation\n    def observation(self, observation):\n        observation = self.permute_orientation(observation)\n        transform = T.Grayscale()\n        observation = transform(observation)\n        return observation\n\nclass ResizeObservation(gym.ObservationWrapper):\n    def __init__(self, env, shape):\n        super().__init__(env)\n        self.shape = (shape, shape) if isinstance(shape, int) else tuple(shape)\n        obs_shape = self.shape + self.observation_space.shape[2:]\n        self.observation_space = Box(low=0, high=255, shape=obs_shape, dtype=np.uint8)\n    def observation(self, observation):\n        transforms = T.Compose([T.Resize(self.shape, antialias=True), T.Normalize(0, 255)])\n        observation = transforms(observation).squeeze(0)\n        return observation\n\n# 2. Model\nclass MarioNet(nn.Module):\n    def __init__(self, input_dim, output_dim):\n        super().__init__()\n        c, h, w = input_dim\n        self.online = nn.Sequential(\n            nn.Conv2d(in_channels=c, out_channels=32, kernel_size=8, stride=4), nn.ReLU(),\n            nn.Conv2d(in_channels=32, out_channels=64, kernel_size=4, stride=2), nn.ReLU(),\n            nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, stride=1), nn.ReLU(),\n            nn.Flatten(),\n            nn.Linear(3136, 512), nn.ReLU(),\n            nn.Linear(512, output_dim),\n        )\n    def forward(self, input, model):\n        return self.online(input)\n\nclass Mario:\n    def __init__(self, state_dim, action_dim):\n        self.state_dim = state_dim\n        self.action_dim = action_dim\n        self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n        self.net = MarioNet(self.state_dim, self.action_dim).float().to(self.device)\n        self.exploration_rate = 0.0\n\n    def act(self, state):\n        state = state[0].__array__() if isinstance(state, tuple) else state.__array__()\n        state = torch.tensor(state, device=self.device).unsqueeze(0)\n        action_values = self.net(state, model=\"online\")\n        return torch.argmax(action_values, axis=1).item()\n\n# 3. Setup\nif gym.__version__ < '0.26':\n    env = gym_super_mario_bros.make(\"SuperMarioBros-1-1-v0\", new_step_api=True)\nelse:\n    env = gym_super_mario_bros.make(\"SuperMarioBros-1-1-v0\", render_mode='rgb', apply_api_compatibility=True)\nenv = JoypadSpace(env, [[\"right\"], [\"right\", \"A\"]])\nenv = SkipFrame(env, skip=4)\nenv = GrayScaleObservation(env)\nenv = ResizeObservation(env, shape=84)\nif gym.__version__ < '0.26':\n    env = FrameStack(env, num_stack=4, new_step_api=True)\nelse:\n    env = FrameStack(env, num_stack=4)\n\nmario = Mario(state_dim=(4, 84, 84), action_dim=env.action_space.n)\n\n\nprint(\"Loading weights...\")\ntry:\n    checkpoint = torch.load(\"/kaggle/working/mario_net_checkpoint.pth\", map_location=\"cpu\")\n    \n    mario.net.load_state_dict(checkpoint, strict=False)\n    \n    mario.net.eval()\n    print(\" Weights loaded \")\nexcept FileNotFoundError:\n    print(\"Checkpoint not found. Running with random weights.\")\nexcept Exception as e:\n    print(f\" An unexpected error occurred: {e}\")\n\n# 5. Record Long Video\nprint(\" Recording 1 minute of gameplay \")\nframes = []\nstate = env.reset()\nif isinstance(state, tuple): state = state[0]\n\nfor i in range(4000): \n    try:\n        frame = env.unwrapped.render(mode='rgb_array')\n    except:\n        frame = env.render()\n        \n    if frame is not None:\n        frames.append(np.copy(frame))\n    \n    action = mario.act(state)\n    step_result = env.step(action)\n    \n    if len(step_result) == 5:\n        next_state, reward, done, trunc, info = step_result\n    else:\n        next_state, reward, done, info = step_result\n        \n    state = next_state\n    \n    if i % 200 == 0:\n        print(f\"Captured {i} frames...\")\n\n    if done: \n        print(f\"Mario died/finished at frame {i}\")\n        break\n\nenv.close()\n\nif len(frames) > 0:\n    print(f\"Saving {len(frames)} frames to 'mario_long.gif'...\")\n    imageio.mimsave('mario_long.gif', frames, fps=25)\n    print(\" Run the display cell below to view.\")\nelse:\n    print(\" Error: No frames were captured.\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from IPython.display import Image\nImage(filename='mario_long.gif')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#  **GRAPH**","metadata":{}},{"cell_type":"code","source":"log_path = \"/kaggle/working/training_log.txt\"\n\ndf = pd.read_csv(\n    log_path,\n    names=[\"episode\", \"step\", \"reward\", \"epsilon\"])\n\nplt.figure(figsize=(10, 5))\nplt.plot(df[\"episode\"], df[\"reward\"])\nplt.xlabel(\"Episode\")\nplt.ylabel(\"Episode Reward\")\nplt.title(\"Mario Training: Episode vs Reward\")\nplt.grid(True)\nplt.show()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"window = 50 \ndf[\"reward_ma\"] = df[\"reward\"].rolling(window=window).mean()\n\nplt.figure(figsize=(10, 5))\nplt.plot(df[\"episode\"], df[\"reward\"], alpha=0.3, label=\"Reward (per episode)\")\nplt.plot(df[\"episode\"], df[\"reward_ma\"], label=f\"Reward {window}-ep Moving Avg\")\nplt.xlabel(\"Episode\")\nplt.ylabel(\"Reward\")\nplt.title(\"Mario Training: Episode Reward and Moving Average\")\nplt.legend()\nplt.grid(True)\nplt.show()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df[\"steps_per_episode\"] = df[\"step\"].diff().fillna(df[\"step\"])\nplt.figure(figsize=(10, 5))\nplt.plot(df[\"episode\"], df[\"steps_per_episode\"])\nplt.xlabel(\"Episode\")\nplt.ylabel(\"Steps in Episode (approx)\")\nplt.title(\"Approx. Steps per Episode\")\nplt.grid(True)\nplt.show()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"plt.figure(figsize=(8, 6))\nplt.scatter(df[\"epsilon\"], df[\"reward\"], alpha=0.4)\nplt.xlabel(\"Epsilon\")\nplt.ylabel(\"Reward\")\nplt.title(\"Reward vs Epsilon\")\nplt.grid(True)\nplt.show()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"plt.figure(figsize=(8, 6))\nplt.hist(df[\"reward\"], bins=30)\nplt.xlabel(\"Reward\")\nplt.ylabel(\"Count\")\nplt.title(\"Distribution of Episode Rewards\")\nplt.grid(True)\nplt.show()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\nimport matplotlib.pyplot as plt\n\nmetric_log_path = \"/kaggle/working/log/log\"\n\ndfm = pd.read_csv(\n    metric_log_path,\n    delim_whitespace=True\n)\n\nprint(dfm.head()) \n\nplt.figure(figsize=(10, 5))\nplt.plot(dfm[\"Episode\"], dfm[\"MeanReward\"])\nplt.xlabel(\"Episode\")\nplt.ylabel(\"Mean Reward (last 100)\")\nplt.title(\"Moving Average Reward (MetricLogger)\")\nplt.grid(True)\nplt.show()\n\nplt.figure(figsize=(10, 5))\nplt.plot(dfm[\"Episode\"], dfm[\"MeanLoss\"])\nplt.xlabel(\"Episode\")\nplt.ylabel(\"Mean Loss\")\nplt.title(\"Mean Loss per Episode\")\nplt.grid(True)\nplt.show()\n\nplt.figure(figsize=(10, 5))\nplt.plot(dfm[\"Episode\"], dfm[\"MeanQValue\"])\nplt.xlabel(\"Episode\")\nplt.ylabel(\"Mean Q-Value\")\nplt.title(\"Mean Q-Value per Episode\")\nplt.grid(True)\nplt.show()\n\nplt.figure(figsize=(10, 5))\nplt.plot(dfm[\"Episode\"], dfm[\"MeanLength\"])\nplt.xlabel(\"Episode\")\nplt.ylabel(\"Mean Episode Length\")\nplt.title(\"Mean Episode Length per Episode\")\nplt.grid(True)\nplt.show()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# **CONTINUATION TRAINING**","metadata":{}},{"cell_type":"code","source":"checkpoint_path=\"/kaggle/working/mario_net_checkpoint.pth\"   \n\nprint(\" Loading previous checkpoint...\")\n\ntry:\n    mario.net.load_state_dict(torch.load(checkpoint_path, map_location=device), strict=False)\n    print(\" Checkpoint loaded. Continuing training...\")\nexcept Exception as e:\n    print(\" Could not load checkpoint:\", e)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"save_dir = Path(\"/kaggle/working\")\nlog_file = save_dir / \"training_log.txt\"\nmario = Mario(state_dim=(4, 84, 84), action_dim=env.action_space.n, save_dir=save_dir)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"mario.exploration_rate=0.4773","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"episodes = 8000 \nmore_episodes = 6000 \nprint(f\"ðŸš€ Continuing Training for {more_episodes} additional episodes...\")\n\nfor e in range(episodes, episodes + more_episodes):  \n    state = env.reset()\n    ep_reward = 0\n\n    while True:\n        action = mario.act(state)\n        step_result = env.step(action)\n\n        if len(step_result) == 5:\n            next_state, reward, done, trunc, info = step_result\n        else:\n            next_state, reward, done, info = step_result\n\n        mario.cache(state, next_state, action, reward, done)\n        q, loss = mario.learn()\n\n        ep_reward += reward\n        state = next_state\n\n        if done or info.get(\"flag_get\", False):\n            break\n\n    if e % 20 == 0:\n        print(f\"Episode {e} | Step {mario.curr_step} | Reward {ep_reward:.1f} | Epsilon {mario.exploration_rate:.4f}\")\n        with open(log_file, \"a\") as f:\n            f.write(f\"{e},{mario.curr_step},{ep_reward},{mario.exploration_rate}\\n\")\n\n    if e % 50 == 0:\n        torch.save(mario.net.state_dict(), save_dir / \"mario_net.pth\")\n\n    if e % 500 == 0:\n        torch.save(mario.net.state_dict(), save_dir / f\"mario_ep_{e}.pth\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\ntorch.save(mario.net.state_dict(), \"mario_net_checkpoint.pth\")\nprint(\"Saved mario_net_checkpoint.pth successfully!\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport numpy as np\nimport imageio\nimport warnings\nfrom torchvision import transforms as T\nfrom gym.spaces import Box\nfrom gym.wrappers import FrameStack\nimport gym_super_mario_bros\nfrom gym_super_mario_bros.actions import SIMPLE_MOVEMENT\nfrom nes_py.wrappers import JoypadSpace\nimport gym\n\nwarnings.filterwarnings('ignore')\n\n# 1. Wrappers\nclass SkipFrame(gym.Wrapper):\n    def __init__(self, env, skip):\n        super().__init__(env)\n        self._skip = skip\n    def step(self, action):\n        total_reward = 0.0\n        for i in range(self._skip):\n            obs, reward, done, trunc, info = self.env.step(action)\n            total_reward += reward\n            if done: break\n        return obs, total_reward, done, trunc, info\n\nclass GrayScaleObservation(gym.ObservationWrapper):\n    def __init__(self, env):\n        super().__init__(env)\n        obs_shape = self.observation_space.shape[:2]\n        self.observation_space = Box(low=0, high=255, shape=obs_shape, dtype=np.uint8)\n    def permute_orientation(self, observation):\n        observation = np.transpose(observation, (2, 0, 1))\n        observation = torch.tensor(observation.copy(), dtype=torch.float)\n        return observation\n    def observation(self, observation):\n        observation = self.permute_orientation(observation)\n        transform = T.Grayscale()\n        observation = transform(observation)\n        return observation\n\nclass ResizeObservation(gym.ObservationWrapper):\n    def __init__(self, env, shape):\n        super().__init__(env)\n        self.shape = (shape, shape) if isinstance(shape, int) else tuple(shape)\n        obs_shape = self.shape + self.observation_space.shape[2:]\n        self.observation_space = Box(low=0, high=255, shape=obs_shape, dtype=np.uint8)\n    def observation(self, observation):\n        transforms = T.Compose([T.Resize(self.shape, antialias=True), T.Normalize(0, 255)])\n        observation = transforms(observation).squeeze(0)\n        return observation\n\n# 2. Model\nclass MarioNet(nn.Module):\n    def __init__(self, input_dim, output_dim):\n        super().__init__()\n        c, h, w = input_dim\n        self.online = nn.Sequential(\n            nn.Conv2d(in_channels=c, out_channels=32, kernel_size=8, stride=4), nn.ReLU(),\n            nn.Conv2d(in_channels=32, out_channels=64, kernel_size=4, stride=2), nn.ReLU(),\n            nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, stride=1), nn.ReLU(),\n            nn.Flatten(),\n            nn.Linear(3136, 512), nn.ReLU(),\n            nn.Linear(512, output_dim),\n        )\n    def forward(self, input, model):\n        return self.online(input)\n\nclass Mario:\n    def __init__(self, state_dim, action_dim):\n        self.state_dim = state_dim\n        self.action_dim = action_dim\n        self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n        self.net = MarioNet(self.state_dim, self.action_dim).float().to(self.device)\n        self.exploration_rate = 0.0\n\n    def act(self, state):\n        state = state[0].__array__() if isinstance(state, tuple) else state.__array__()\n        state = torch.tensor(state, device=self.device).unsqueeze(0)\n        action_values = self.net(state, model=\"online\")\n        return torch.argmax(action_values, axis=1).item()\n\n\nif gym.__version__ < '0.26':\n    env = gym_super_mario_bros.make(\"SuperMarioBros-1-1-v0\", new_step_api=True)\nelse:\n    env = gym_super_mario_bros.make(\"SuperMarioBros-1-1-v0\", render_mode='rgb', apply_api_compatibility=True)\nenv = JoypadSpace(env, [[\"right\"], [\"right\", \"A\"]])\nenv = SkipFrame(env, skip=4)\nenv = GrayScaleObservation(env)\nenv = ResizeObservation(env, shape=84)\nif gym.__version__ < '0.26':\n    env = FrameStack(env, num_stack=4, new_step_api=True)\nelse:\n    env = FrameStack(env, num_stack=4)\n\nmario = Mario(state_dim=(4, 84, 84), action_dim=env.action_space.n)\n\n\nprint(\"Loading weights...\")\ntry:\n    checkpoint = torch.load(\"/kaggle/working/mario_net_checkpoint.pth\", map_location=\"cpu\")\n    \n    \n    mario.net.load_state_dict(checkpoint, strict=False)\n    \n    mario.net.eval()\n    print(\" Weights loaded! (Ignored unnecessary 'target' layers)\")\nexcept FileNotFoundError:\n    print(\"âš ï¸ Checkpoint not found. Running with random weights.\")\nexcept Exception as e:\n    print(f\" An unexpected error occurred: {e}\")\n\n# 5. Record Long Video\nprint(\"ðŸŽ¥ Recording 1 minute of gameplay (approx 2000 frames)...\")\nframes = []\nstate = env.reset()\nif isinstance(state, tuple): state = state[0]\n\nfor i in range(4000): \n    try:\n        frame = env.unwrapped.render(mode='rgb_array')\n    except:\n        frame = env.render()\n        \n    if frame is not None:\n        frames.append(np.copy(frame))\n    \n    action = mario.act(state)\n    step_result = env.step(action)\n    \n    if len(step_result) == 5:\n        next_state, reward, done, trunc, info = step_result\n    else:\n        next_state, reward, done, info = step_result\n        \n    state = next_state\n    \n    if i % 200 == 0:\n        print(f\"Captured {i} frames...\")\n\n    if done: \n        print(f\"Mario died/finished at frame {i}\")\n        break\n\nenv.close()\n\nif len(frames) > 0:\n    print(f\" Saving {len(frames)} frames to 'mario_long.gif'...\")\n    imageio.mimsave('mario_long.gif', frames, fps=25)\n    print(\" Done! Run the display cell below to view.\")\nelse:\n    print(\" Error: No frames were captured.\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from IPython.display import Image\nImage(filename='/kaggle/working/mario_long.gif')","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}